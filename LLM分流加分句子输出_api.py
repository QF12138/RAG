from openai import OpenAI
import json
import re
import os
# å…¶ä½™ä¾èµ–ï¼ˆå¦‚ sentence_transformers, langchain ç­‰ï¼‰å’ŒåŸæ¥ä¿æŒä¸€è‡´
import torch
from unsloth import FastLanguageModel
from langchain.docstore.document import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.ensemble import EnsembleRetriever
from sentence_transformers import CrossEncoder
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

def _call_deepseek_api(prompt: str ,api_key:str) -> str:
    """
    ä½¿ç”¨ DeepSeek API æ¥å®Œæˆå¯¹è¯è¡¥å…¨ï¼Œè¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ã€‚
    è¿™é‡Œçš„ prompt å¯ä»¥æ˜¯ç³»ç»Ÿ+ç”¨æˆ·çš„ç»„åˆï¼Œä¹Ÿå¯ä»¥åªåŒ…å«ç”¨æˆ·éƒ¨åˆ†ï¼Œ
    æ ¹æ®å®é™…éœ€è¦è‡ªè¡Œè°ƒæ•´ã€‚
    """
    # å®ä¾‹åŒ–å®¢æˆ·ç«¯
    # api_key å’Œ base_url æ›¿æ¢ä¸ºä½ åœ¨ DeepSeek ä¸Šçš„é…ç½®
    client = OpenAI(
        api_key= api_key,
        base_url="https://api.deepseek.com"
    )

    # è°ƒç”¨ chat.completions æ¥å£
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šåœ°è´¨é¢†åŸŸAI"},
                {"role": "user", "content": prompt},
            ],
            stream=False  # å¦‚æœéœ€è¦æµå¼æ¨é€ï¼Œå¯æ”¹ä¸º True
        )
        # è¿”å›ç”Ÿæˆç»“æœ
        return response.choices[0].message.content
    except Exception as e:
        return f"DeepSeek API è°ƒç”¨å¼‚å¸¸: {str(e)}"

class AutoJSONProcessor:
    """
    è‡ªåŠ¨å¤„ç† JSON æ–‡ä»¶çš„åŠ è½½å™¨ï¼š
    - é€’å½’æå– JSON æ•°æ®ä¸­æ‰€æœ‰å­—ç¬¦ä¸²å†…å®¹ã€‚
    - å°†æå–çš„æ–‡æœ¬æ‹¼æ¥æˆæ•´ä½“ï¼Œç”Ÿæˆ Document å¯¹è±¡ã€‚
    """
    def __init__(self, file_path: str, encoding: str = "utf-8"):
        self.file_path = file_path
        self.encoding = encoding

    @staticmethod
    def extract_text(data):
        texts = []
        if isinstance(data, str):
            texts.append(data)
        elif isinstance(data, dict):
            for value in data.values():
                texts.extend(AutoJSONProcessor.extract_text(value))
        elif isinstance(data, list):
            for item in data:
                texts.extend(AutoJSONProcessor.extract_text(item))
        return texts

    def load(self):
        with open(self.file_path, "r", encoding=self.encoding) as f:
            data = json.load(f)
        texts = self.extract_text(data)
        content = " ".join(texts).strip()
        documents = []
        if content:
            documents.append(Document(page_content=content, metadata={"source": self.file_path}))
        return documents


class RecursiveLoader:
    def __init__(self, directory: str, file_extension: str = "*.json"):
        self.directory = directory
        self.file_extension = file_extension

    def load_files(self):
        loader = DirectoryLoader(self.directory, glob=f"**/{self.file_extension}", loader_cls=AutoJSONProcessor)
        return loader.load()


class SmartDocumentProcessor:
    def __init__(self):
        # è¿™é‡Œä¿ç•™ Embedding æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ° bge-small-zh-v1.5ï¼‰ï¼Œä¾› SemanticChunker ç”¨
        model_local_path = os.path.abspath("../../models/bge-small-zh-v1.5")
        self.embed_model = HuggingFaceEmbeddings(
            model_name=model_local_path,
            model_kwargs={"local_files_only": True, "device": "cuda"},
            encode_kwargs={"batch_size": 16}
        )

    def _detect_content_type(self, text):
        if re.search(r'def |import |print\(|ä»£ç ç¤ºä¾‹', text):
            return "code"
        elif re.search(r'\|.+\|', text) and '%' in text:
            return "table"
        return "normal"

    def process_documents(self):
        loaders = [
            DirectoryLoader("../../knowledge_base", glob="**/*.pdf", loader_cls=PyPDFLoader),
            DirectoryLoader("../../knowledge_base", glob="**/*.txt", loader_cls=TextLoader,
                            loader_kwargs={"encoding": "utf-8"}),
            DirectoryLoader("../../knowledge_base", glob="**/*.json", loader_cls=AutoJSONProcessor,
                            loader_kwargs={"encoding": "utf-8"})
        ]
        documents = []
        for loader in loaders:
            documents.extend(loader.load())

        # å…ˆåšè¯­ä¹‰çº§åˆ«çš„åˆ†å—
        chunker = SemanticChunker(
            embeddings=self.embed_model,
            breakpoint_threshold_amount=82,
            add_start_index=True
        )
        base_chunks = chunker.split_documents(documents)

        final_chunks = []
        for chunk in base_chunks:
            content_type = self._detect_content_type(chunk.page_content)
            if content_type == "code":
                splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=64)
            elif content_type == "table":
                splitter = RecursiveCharacterTextSplitter(chunk_size=384, chunk_overlap=96)
            else:
                splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)
            final_chunks.extend(splitter.split_documents([chunk]))

        for i, chunk in enumerate(final_chunks):
            chunk.metadata.update({
                "chunk_id": f"chunk_{i}",
                "content_type": self._detect_content_type(chunk.page_content)
            })
        return final_chunks


class HybridRetriever:
    def __init__(self, chunks):
        # æ„å»ºå‘é‡æ•°æ®åº“
        self.vector_db = Chroma.from_documents(
            chunks,
            embedding=HuggingFaceEmbeddings(
                model_name="../../models/bge-large-zh-v1.5",
                model_kwargs={"local_files_only": True, "device": "cuda"}
            ),
            persist_directory="../../vector_db"
        )

        # BM25
        self.bm25_retriever = BM25Retriever.from_documents(chunks, k=5)
        # Ensemble: å‘é‡æ£€ç´¢ + BM25
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[
                self.vector_db.as_retriever(search_kwargs={"k": 5}),
                self.bm25_retriever
            ],
            weights=[0.6, 0.4]
        )

        # CrossEncoder äº¤äº’å¼é‡æ’
        self.reranker = CrossEncoder("../../models/bge-reranker-large", device="cuda")

    def retrieve(self, query, top_k=5):
        docs = self.ensemble_retriever.invoke(query)
        pairs = [[query, doc.page_content] for doc in docs]
        scores = self.reranker.predict(pairs)
        ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked_docs[:top_k]]

class EnhancedRAG:
    def __init__(self):
        """
        å‚æ•°:
          api_key: DeepSeek API key
        """
        # ç¬¬ä¸€æ­¥ï¼šæ„å»ºæ–‡æ¡£æ£€ç´¢å™¨
        processor = SmartDocumentProcessor()
        chunks = processor.process_documents()
        self.retriever = HybridRetriever(chunks)

        # å¦‚æœæœ‰è¿ç¦è¯æ–‡ä»¶ï¼Œå°±åŠ è½½è¿›æ¥ï¼›æ²¡æœ‰å°±ç©ºç½®
        try:
            with open("../../banned_keywords.TXT", "r", encoding="utf-8") as f:
                self.banned_keywords = f.read().split()
        except FileNotFoundError:
            self.banned_keywords = []

        # ä¿å­˜ DeepSeek API keyï¼Œä¾›åç»­è°ƒç”¨
        self.api_key = "sk-996a6e8eeec94ce4be8d44acb3f40189"

    def is_geological_question(self, user_input):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåœ°è´¨ç›¸å…³é—®é¢˜"""
        geo_keywords = ["å²©ä½“", "å›´å²©", "éš§é“", "åœ°è´¨", "èŠ‚ç†", "åœ°ä¸‹æ°´", "å¼€æŒ–", "æŒå­é¢"]
        return any(keyword in user_input for keyword in geo_keywords)

    def contains_banned_content(self, text: str) -> bool:
        return any(keyword in text for keyword in self.banned_keywords)

    def parse_input_to_categories(self, user_text: str, type: str) -> dict:

        # æ„é€ æç¤ºè¯ï¼Œè¦æ±‚æ¨¡å‹åˆ†ç±»+èåˆæ¯ç±»å¥å­ä¸ºä¸€æ®µè¯
        prompt = f"""
    è¯·å¯¹å¦‚ä¸‹ä¿¡æ¯æŒ‰åˆ†ç±»æ–¹æ³•è¿›è¡Œç†è§£åˆ†ç±»ï¼š{user_text}
    è¦æ±‚é€‰å‡ºä½ è®¤ä¸ºå’Œ{type}æœ€ç›¸å…³çš„å¥å­ï¼Œå¹¶åˆå¹¶ä¸ºä¸€å¥è¯ã€‚

    åˆ†ç±»åŒ…æ‹¬ä»¥ä¸‹ä¸‰ç±»ï¼š
    1. å²©çŸ³åšç¡¬ç¨‹åº¦ï¼‰ï¼šåˆ¤æ–­æ ‡å‡†ï¼šå²©çŸ³åšç¡¬ç¨‹åº¦ç”±å²©æ€§ã€é£åŒ–ä½œç”¨ã€æ°´ä½œç”¨ä¸‰è€…å…±åŒå†³å®šã€‚
    2. å²©ä½“å®Œæ•´ç¨‹åº¦åˆ¤æ–­æ ‡å‡†ï¼šç»“æ„é¢å‘è‚²ç¨‹åº¦ã€ç»“åˆç¨‹åº¦ã€ç»“æ„é¢ç±»å‹ã€å²©ä½“ç»“æ„ç±»å‹ã€‚
    3. åœ°ä¸‹æ°´åˆ¤æ–­æ ‡å‡†ï¼šå¥å­ä¸­æ¶‰åŠæ¸—æ°´ã€å‡ºæ°´ã€æ½®æ¹¿ã€æ¹¿æ¶¦ã€æ»´æ°´ç­‰æè¿°ã€‚

    æ³¨æ„äº‹é¡¹ï¼š
    - æ¯ä¸ªå¥å­å¯ä»¥å±äºå¤šä¸ªç±»åˆ«ï¼›
    - æ¯ä¸€ç±»è¯·æ•´åˆå¹¶æ¶¦è‰²æ‰€æœ‰ç›¸å…³å¥å­ï¼Œå½¢æˆä¸€æ®µå®Œæ•´è¡¨è¾¾ï¼›
    """
        # è°ƒç”¨æ¨¡å‹
        try:
            answer = _call_deepseek_api(prompt, self.api_key)
            return answer
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

    def _call_model_for_category(self, category_text: str, category_name: str, extra_instructions: str) -> str:
        """
        é€šç”¨çš„å‡½æ•°ï¼šç»™å®šç”¨æˆ·æ–‡æœ¬(æŸä¸€ç±») + æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ + ç‰¹å®šæ ¼å¼æç¤ºï¼Œ
        ç”¨ DeepSeek API è·å–ç»“æœã€‚
        """
        if not category_text.strip():
            return "ï¼ˆæ— ç›¸å…³ä¿¡æ¯ï¼‰"

        # 1) ç”¨ç”¨æˆ·æ–‡æœ¬æ‹¼å‡ºæ£€ç´¢ query
        category_query = f"{category_text}\nä¸{category_name}ç›¸å…³"

        # 2) æ£€ç´¢
        retrieved_docs = self.retriever.retrieve(category_query, top_k=3)
        context_str = "\n\n".join(
            [f"[æ¥æºï¼š{doc.metadata.get('source', 'unknown')}]\n{doc.page_content}" for doc in retrieved_docs]
        )

        # 3) æ„é€  Prompt
        prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šåœ°è´¨é¢†åŸŸAIã€‚ä»¥ä¸‹æ˜¯ç”¨æˆ·æ‹†åˆ†å‡ºçš„ä¸ã€{category_name}ã€‘ç›¸å…³ä¿¡æ¯ï¼š
{category_text}

è¿™æ˜¯æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™ï¼š
{context_str}

ç°åœ¨è¯·ä½ å¯¹ã€{category_name}ã€‘ä½œå‡ºä¸€ä¸ªä¸“ä¸šåˆ¤æ–­ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚
è¯·åŠ¡å¿…è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼ï¼Œå½¢å¦‚ï¼š
{{
  "ï¼ˆåˆ¤å®šå¯¹è±¡ï¼‰": "è¿™é‡Œå†™æœ€åç»“è®º",
  "ç†ç”±": "è¿™é‡Œå†™ä¾æ®ç›¸å…³ä¿¡æ¯å’Œå‚è€ƒèµ„æ–™çš„æ€è€ƒè¿‡ç¨‹"
}}
ä»¥ä¸‹å†…å®¹ä¸ºæ³¨æ„äº‹é¡¹ï¼š
{extra_instructions}
        """

        # 4) è°ƒç”¨ DeepSeek API å®Œæˆç”Ÿæˆ
        answer = _call_deepseek_api(prompt, self.api_key)
        return answer

    def analyze_and_stitch(self, user_text: str) -> list:
        rock_text = self.parse_input_to_categories(user_text, 'å²©çŸ³åšç¡¬ç¨‹åº¦')
        integrity_text = self.parse_input_to_categories(user_text, "å²©ä½“å®Œæ•´ç¨‹åº¦")
        water_text = self.parse_input_to_categories(user_text, "åœ°ä¸‹æ°´")

        # åˆ†åˆ«ç»™å‡ºä¸‰ç§åœºæ™¯ä¸‹çš„ä¸åŒæç¤ºè¦æ±‚
        rock_ans = self._call_model_for_category(
            rock_text,
            "å²©çŸ³åšç¡¬ç¨‹åº¦",
            """
            1.å²©çŸ³åšç¡¬ç¨‹åº¦ç”±å²©æ€§ã€é£åŒ–ä½œç”¨å’Œæ°´ä½œç”¨ä¸‰è€…å…±åŒå†³å®šï¼›2.ç†ç”±è¦å……åˆ†ç»“åˆèµ„æ–™ï¼›3.åªè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚
            è¾“å‡ºç»“æœåœ¨å¦‚ä¸‹ä¸­é€‰æ‹©æ ¹æ®ç»“æœé€‰æ‹©å¯¹åº”çš„æ•°å­—ï¼Œåªé€‰ä¸€é¡¹ä¸è¦èŒƒå›´ï¼š
            åšç¡¬å²©ï¼š1ï¼›è¾ƒåšç¡¬å²©ï¼š2ï¼›è¾ƒè½¯å²©ï¼š3ï¼›è½¯å²©ï¼š4ï¼›æè½¯å²©ï¼š5ï¼›
            """
        )
        integrity_ans = self._call_model_for_category(
            integrity_text,
            "å²©ä½“å®Œæ•´ç¨‹åº¦",
            """
            1.å²©ä½“å®Œæ•´åº¦ä¸»è¦çœ‹ç»“æ„é¢å‘è‚²ç¨‹åº¦ï¼›2.ç†ç”±è¦å……åˆ†ç»“åˆèµ„æ–™ï¼›3.åªè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚
            è¾“å‡ºç»“æœåœ¨å¦‚ä¸‹ä¸­é€‰æ‹©æ ¹æ®ç»“æœé€‰æ‹©å¯¹åº”çš„æ•°å­—ï¼Œåªé€‰ä¸€é¡¹ä¸è¦èŒƒå›´ï¼š
            å®Œæ•´ï¼š1ï¼›è¾ƒå®Œæ•´ï¼š2ï¼›è¾ƒç ´ç¢ï¼š3ï¼›ç ´ç¢ï¼š4ï¼›æç ´ç¢ï¼š5ï¼›
            """
        )
        water_ans = self._call_model_for_category(
            water_text,
            "åœ°ä¸‹æ°´",
            "1.åœ°ä¸‹æ°´çŠ¶æ€çš„åˆ¤å®šç»“æœè¦ä¸ºâ… ,â…¡ç­‰ç­‰çº§å¹¶å°†ç½—é©¬æ•°å­—æ¢æˆé˜¿æ‹‰ä¼¯æ•°å­—è¾“å‡ºï¼›2.ç†ç”±è¦å……åˆ†ç»“åˆèµ„æ–™ï¼›3.åªè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚"
        )

        # ç®€æ˜“æå–å‡½æ•°ï¼šä»æ–‡æœ¬ä¸­æ‰¾å‡º JSON å—
        def extract_json_list(text: str) -> list:
            json_blocks = re.findall(r'({.*?})', text, re.DOTALL)
            json_list = []
            for block in json_blocks:
                try:
                    json_obj = json.loads(block)
                    json_list.append(json_obj)
                except json.JSONDecodeError:
                    pass
            return json_list

        # æœ€åç»Ÿä¸€æ‹¼åˆ°ä¸€èµ·ï¼Œå†å°è¯•è§£ææ‰€æœ‰ JSON
        final_result = f"{rock_ans}\n{integrity_ans}\n{water_ans}"
        return extract_json_list(final_result)

    def ask(self, user_input: str) -> str:
        # è¿ç¦è¯æ£€æµ‹
        if self.contains_banned_content(user_input):
            return "âš ï¸ è¯¥å†…å®¹æ¶‰åŠè¿è§„è¯é¢˜ï¼Œæ— æ³•å›ç­”"
        if not self.is_geological_question(user_input):
            prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªé€šç”¨æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä»¥ç®€æ´ã€æ¸…æ™°ã€å‹å¥½çš„æ–¹å¼å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š
        {user_input}
        """
            answer = _call_deepseek_api(prompt, self.api_key)
            return answer
        else:
            # è°ƒç”¨ä¸‰åˆ†ç±» + æ£€ç´¢ + DeepSeek API
            result = self.analyze_and_stitch(user_input)
            return json.dumps(result, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    rag = EnhancedRAG()

    print("è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰")
    while True:
        user_input = input("\n>>> ç”¨æˆ·: ").strip()
        if user_input.lower() == 'quit':
            print("\nå¯¹è¯ç»“æŸ")
            break

        if not user_input:
            print("é—®é¢˜ä¸èƒ½ä¸ºç©º")
            continue

        print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...", end="", flush=True)
        answer = rag.ask(user_input)
        print("\r" + " " * 20 + "\r", end="")
        print("\nğŸ’¡ ç³»ç»Ÿå›ç­”:")
        print("-" * 50)
        print(answer)
        print("-" * 50)
